### **13.1 文件 I/O 的内核缓冲：缓冲区高速缓存**

`read()` 和 `write()` 系统调用在操作磁盘文件时不会直接发起磁盘访问，而是仅仅在用户空间缓冲区与**内核缓冲区高速缓存（kernel buffer cache）**之间复制数据。

例如，对于 `write()` 调用，数据会从用户空间传递到内核空间的缓冲区中，然后 `write()` 随即返回。在后续某个时刻，内核会将其缓冲区中的数据写入（刷新至）磁盘。这个过程是**异步**的。如果在此期间，另一进程试图读取该文件的这几个字节，那么内核将自动从缓冲区高速缓存中提供这些数据，而不是从文件中读取过期的内容。

与此同理，对于 `read()` 调用，内核从磁盘中读取数据并存储到内核缓冲区中。`read()` 调用将从该缓冲区中读取数据，直至把缓冲区中的数据取完，这时，内核会将文件的下一段内容读入缓冲区高速缓存。（对于序列化的文件访问，内核通常会尝试执行**预读（read-ahead）**，以确保在需要之前就将文件的下一数据块读入缓冲区高速缓存中。）

采用这一设计，意在使 `read()` 和 `write()` 调用的操作更为快速，因为它们不需要等待缓慢的磁盘操作。同时，这一设计也极为高效，因为它减少了内核必须执行的磁盘传输次数。

Linux 内核对缓冲区高速缓存的大小没有固定上限。内核会分配尽可能多的物理内存用于缓冲区高速缓存。若可用内存不足，则内核会将一些修改过的（"脏"的）缓冲区高速缓存页内容刷新到磁盘，并释放其内存供系统重用。

> 从内核 2.4 开始，Linux 不再维护一个单独的缓冲区高速缓存。相反，文件 I/O 缓冲区被置于**页面高速缓存（page cache）**中，其中还含有诸如内存映射文件的页面。然而，本文仍采用“缓冲区高速缓存”这一术语，因为这是 UNIX 实现中历史悠久的通称。

#### **缓冲区大小对 I/O 系统调用性能的影响**

无论是让磁盘写 1000 次、每次写入一个字节，还是一次写入 1000 个字节，内核访问磁盘的字节数都是相同的。然而，我们更倾向于后者，因为它只需要一次系统调用，而前者则需要 1000 次。尽管比磁盘操作快许多，但系统调用本身也需要耗费相当可观的时间。

通过为 `BUF_SIZE`（每次 `read()`/`write()` 传输的字节数）设定不同的大小，可以观察到不同缓冲区对文件 I/O 性能的影响。下表所示为在 Linux ext2 文件系统上复制一个 100MB 大小的文件时，使用不同 `BUF_SIZE` 值所需的时间。

**表 13-1：复制 100MB 大小的文件所需时间**
| BUF_SIZE | 总用时(s) | 总 CPU 用时(s) | 用户 CPU 用时(s) | 系统 CPU 用时(s) |
|:---|:---|:---|:---|:---|
| 1 | 107.43 | 107.32 | 8.20 | 99.12 |
| 2 | 54.16 | 53.89 | 4.13 | 49.76 |
| 4 | 31.72 | 30.96 | 2.30 | 28.66 |
| 8 | 15.59 | 14.34 | 1.08 | 13.26 |
| 16 | 7.50 | 7.14 | 0.51 | 6.63 |
| 32 | 3.76 | 3.68 | 0.26 | 3.41 |
| 64 | 2.19 | 2.04 | 0.13 | 1.91 |
| 128 | 2.16 | 1.59 | 0.11 | 1.48 |
| 256 | 2.06 | 1.75 | 0.10 | 1.65 |
| 512 | 2.06 | 1.03 | 0.05 | 0.98 |
| 1024 | 2.05 | 0.65 | 0.02 | 0.63 |
| 4096 | 2.05 | 0.38 | 0.01 | 0.38 |
| 16384 | 2.05 | 0.34 | 0.00 | 0.33 |
| 65536 | 2.06 | 0.32 | 0.00 | 0.32 |

从表中可以看出，当缓冲区大小很小时，系统调用开销极大，导致性能很差。随着缓冲区增大，性能显著提升。当缓冲区大小达到 4096 字节（文件系统块大小）左右时，性能趋于稳定，因为此时系统调用的成本相对于数据复制和实际磁盘 I/O 的时间已经微不足道了。

#### **写操作的缓冲效果**

表 13-1 度量了读系统调用、写系统调用、内核与用户空间数据传输以及磁盘读写等一系列因素。由于 `write()` 调用会立即返回（数据被写入缓冲区高速缓存），可以推断，当程序完成时，输出文件实际尚未完全写入磁盘。因此，表 13-1 中的总耗时主要由**磁盘读取**操作决定。

为了验证这一点，下面是另一个实验的结果，该实验只向一个 100MB 的文件**写入**数据。

**表 13-2：写一个 100MB 大小的文件所需要的时间**
| BUF_SIZE | 总用时(s) | 总 CPU 用时(s) | 用户 CPU 用时(s) | 系统 CPU 用时(s) |
|:---|:---|:---|:---|:---|
| 1 | 72.13 | 72.11 | 5.00 | 67.11 |
| 2 | 36.19 | 36.17 | 2.47 | 33.70 |
| 4 | 20.01 | 19.99 | 1.26 | 18.73 |
| 8 | 9.35 | 9.32 | 0.62 | 8.70 |
| 16 | 4.70 | 4.68 | 0.31 | 4.37 |
| 32 | 2.39 | 2.39 | 0.16 | 2.23 |
| 64 | 1.24 | 1.24 | 0.07 | 1.16 |
| 128 | 0.67 | 0.67 | 0.04 | 0.63 |
| 256 | 0.38 | 0.38 | 0.02 | 0.36 |
| 512 | 0.24 | 0.24 | 0.01 | 0.23 |
| 1024 | 0.17 | 0.17 | 0.01 | 0.16 |
| 4096 | 0.11 | 0.11 | 0.00 | 0.11 |
| 16384 | 0.10 | 0.10 | 0.00 | 0.10 |
| 65536 | 0.09 | 0.09 | 0.00 | 0.09 |

表 13-2 显示了仅将数据从用户空间传输到内核缓冲区高速缓存所花费的成本。对于 65536 字节的大缓冲区，总耗时仅为 0.09 秒，远低于表 13-1 中的 2.06 秒。这证明了在前者（文件复制）的情况下，绝大部分时间都花在了对磁盘的读取上，而写操作因为被内核缓冲而显得非常快。

总之，如果与文件发生大量的数据传输，通过采用大块空间缓冲数据并执行更少的系统调用，可以极大地提高 I/O 性能。