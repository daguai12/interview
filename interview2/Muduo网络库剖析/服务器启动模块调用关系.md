# 服务器启动所涉及的模块调用关系图


![[Pasted image 20250527165435.png]]
# 从threadPool_->start()到Thread::start()所经历的过程

## TcpServer::Start()
```cpp
void TcpServer::start(){
	threadPool_->start(threadInitCallback_); 
	...
}
```

在函数`start()`中，`threadPool_`成员变量调用`start(threadInitCallback_)`方法,来启动线程池。

## EventLoopThreadPool::start()
```cpp
void EventLoopThreadPool::start(const ThreadInitCallback& cb)
{
    started_ = true;
    for (int i = 0; i < numThreads_; ++i)
    {
        char buf[name_.size() + 32];
        snprintf(buf, sizeof buf, "%s%d",name_.c_str(),i);
        EventLoopThread *t = new EventLoopThread(cb,buf);
        threads_.push_back(std::unique_ptr<EventLoopThread>(t));
        loops_.push_back(t->startLoop());
    }

    // 整个服务端只有一个线程，运行着baseloop
    if (numThreads_ == 0 && cb)
    {
        cb(baseLoop_);
    }
}
```

这个函数用于**启动线程池中的所有线程**，并且为每个线程创建一个 `EventLoop` 实例。

* 每个 `EventLoop` 都在独立的线程中运行，监听和处理 IO 事件。
* 如果没有开启多线程（`numThreads_ == 0`），就直接在主线程（baseloop）里运行回调。

### 函数逐句剖析：

```cpp
void EventLoopThreadPool::start(const ThreadInitCallback& cb)
```

* `start`：启动线程池，创建并启动线程。
* `cb`：可选的线程初始化回调，在每个新线程的 `EventLoop` 启动前调用。

### 启动标志

```cpp
started_ = true;
```

* 标记线程池已启动，避免重复启动。


### 启动 `numThreads_` 个线程

```cpp
for (int i = 0; i < numThreads_; ++i)
```

循环创建 `numThreads_` 个 `EventLoopThread`。


### 设置线程名

```cpp
char buf[name_.size() + 32];
snprintf(buf, sizeof buf, "%s%d", name_.c_str(), i);
```

* 给每个线程生成唯一的名字，比如 `IOThread0`, `IOThread1`，便于调试和日志。


### 创建并保存线程对象

```cpp
EventLoopThread *t = new EventLoopThread(cb, buf);
threads_.push_back(std::unique_ptr<EventLoopThread>(t));
```

* new 出一个 `EventLoopThread`，传入初始化回调和线程名。
* 用 `unique_ptr` 管理，防止内存泄漏。


### 启动线程，获取子线程中的 EventLoop

```cpp
loops_.push_back(t->startLoop());
```

* `startLoop()` 会：

  1. 启动新线程。
  2. 在新线程中创建一个 `EventLoop`。
  3. 通知主线程返回这个 `EventLoop*`。
* 保存这个 `EventLoop*` 到 `loops_` 容器，便于后续在 `TcpServer` 中分发连接（轮询调度 IO 事件时用）。


### 单线程模式下执行回调

```cpp
if (numThreads_ == 0 && cb)
{
    cb(baseLoop_);
}
```

* 如果没设置多线程，直接对主线程（`baseLoop_`）执行初始化回调。



### 为什么这么设计？

* **主线程 baseLoop\_**：监听 accept、分发新连接。
* **子线程池中的 EventLoop**：监听连接上的 IO 事件（读写、关闭等）。
* 保证一个线程一个 EventLoop，一个 EventLoop 只在自己线程里运行，**避免锁**，简化并发模型。

## EventLoopThread::startLoop()  

```cpp
EventLoop* EventLoopThread::startLoop()
{
    thread_.start();

    EventLoop *loop = nullptr;
    {
        std::unique_lock<std::mutex> lock(mutex_);
        while ( loop_ == nullptr )
        {
            cond_.wait(lock);
        }
        loop = loop_;
    }
    return loop;
}
```


### 函数作用：

👉 启动 `EventLoopThread` 线程，并等待线程内初始化好 `EventLoop`，然后安全地返回这个 `EventLoop*` 指针给主线程。



### 1️⃣ 启动线程

```cpp
thread_.start();
```

* `thread_` 是 `muduo::Thread` 类型（封装了 `pthread_create`）。
* 调用 `start()` 会新建一个线程，线程执行 `EventLoopThread::threadFunc()`，**在那个线程里会创建 EventLoop 实例**。
* ⚠️ 这时候，子线程刚启动，EventLoop 还没初始化好。

---

### 2️⃣ 主线程声明一个 `loop` 临时变量

```cpp
EventLoop *loop = nullptr;
```

---

### 3️⃣ 加锁等待

```cpp
{
    std::unique_lock<std::mutex> lock(mutex_);
```

* 保证访问 `loop_` 是线程安全的。

---

### 4️⃣ 等待子线程信号

```cpp
while (loop_ == nullptr)
{
    cond_.wait(lock);
}
```

* 如果 `loop_` 还没初始化（此时子线程里的 EventLoop 可能还没 new 出来）
* **主线程阻塞在这里，等子线程通过 `cond_.notify_one()` 通知它。**


### 5️⃣ 拿到 loop\_，退出临界区

```cpp
loop = loop_;
```

* 子线程在执行 `threadFunc()` 时，创建 EventLoop 实例，并赋值给 `loop_`，然后唤醒这个 cond。
* 拿到 EventLoop 指针，退出锁保护范围。


### 6️⃣ 返回 EventLoop\*

```cpp
return loop;
```


### 📌 为什么要这样设计？

* **跨线程通信**：

  * 主线程需要拿到子线程里那个 `EventLoop*`，但 EventLoop 必须在子线程中创建，生命周期由子线程控制。
  * 通过 `mutex_` + `cond_` 做同步，避免主线程在子线程 EventLoop 没建好时就取值，产生空指针或竞态。

* **同步阻塞等待**：

  * 保证 `startLoop()` 返回时，子线程的 `EventLoop` 已经准备好，**可以立即加入 loop 列表，投入使用。**


## Thread::start()

###  作用概述：

这个函数会：

1. 标记线程已启动
2. 创建一个新线程，运行 `func_()`（用户设置的线程函数）
3. 用 `sem_t` 信号量同步，**主线程等待子线程获取自己的 tid 并上报，再继续执行**



### 1️⃣ 设置线程已启动

```cpp
started_ = true;
```

* 标记这个 `Thread` 实例的线程已经启动，避免重复调用。


### 2️⃣ 初始化信号量

```cpp
sem_t sem;
sem_init(&sem, false, 0);
```

* `sem` 是一个 POSIX 信号量，值初始化为 0。
* 第二个参数 `false` 表示是线程间同步（而非进程间）。


### 3️⃣ 创建新线程

```cpp
thread_ = std::shared_ptr<std::thread>(new std::thread([&](){
```

* 新建一个 `std::thread`，线程执行的 lambda 捕获 `sem`、`this` 之类的局部变量。


### 4️⃣ 子线程获取自身 tid

```cpp
tid_ = CurrentThread::tid();
```

* `CurrentThread::tid()` 获取当前线程的线程ID。
* 然后把这个 tid 存到 `Thread` 对象的 `tid_` 成员里，供主线程查看。


### 5️⃣ 通知主线程

```cpp
sem_post(&sem);
```

* 子线程执行到这一步时，说明它已经获取到了自己的 tid。
* `sem_post()` 把信号量 +1，唤醒主线程。


### 6️⃣ 子线程正式开始干活

```cpp
func_();
```

* 执行用户设定的线程函数，比如 `EventLoopThread::threadFunc()`。


### 7️⃣ 主线程阻塞等待

```cpp
sem_wait(&sem);
```

* 主线程阻塞在这里，直到子线程调用 `sem_post()`。
* 确保主线程在**继续往下执行之前，子线程的 tid\_ 已经赋值完毕**。





# 从 loop_->runInLoop到可以监听用户连接所经历的过程


## 先看 `Acceptor::listen()` 源码

```cpp
void Acceptor::listen()
{
    listenning_ = true;
    acceptSocket_.listen();   // 调用 Socket 封装的 listen() 系统调用
    acceptChannel_.enableReading();  // 让 acceptChannel 关注读事件（新连接到来）
}
```



###  ① `acceptSocket_.listen()`

👉 查看 `Socket` 类的 `listen()`

```cpp
void Socket::listen()
{
    ::listen(sockfd_, SOMAXCONN);
}
```

* 调用 Linux 系统调用 `listen()`
* `SOMAXCONN` 是 backlog 队列最大长度
* **让内核监听 listenFd 的可连接队列**

✔️ 到这里，内核就能接收客户端连接了。


### 📦 ② `acceptChannel_.enableReading()`

👉 查看 `Channel` 类的 `enableReading()`

```cpp
void Channel::enableReading()
{
    events_ |= kReadEvent; // kReadEvent = EPOLLIN
    update();
}
```

* 设置监听事件 `EPOLLIN`
* 调用 `update()` 将该 Channel 加入 `EventLoop` 的 `epoll`


### 📦 ③ `Channel::update()`

👉 看 `Channel::update()`

```cpp
void Channel::update()
{
    loop_->updateChannel(this);
}
```

* 通知 `EventLoop`，把当前 `Channel` 添加进 `epoll`


### 📦 ④ `EventLoop::updateChannel()`

👉 看 `EventLoop` 的 `updateChannel()`

```cpp
void EventLoop::updateChannel(Channel* channel)
{
    poller_->updateChannel(channel);
}
```

* `poller_` 就是 epoll 的封装 `EpollPoller`
* 把 `Channel` 注册到 epoll 的关注列表


### 📦 ⑤ `Poller::updateChannel()` → `EpollPoller::updateChannel()`

👉 看 `EpollPoller` 的 `updateChannel()`

```cpp
void EpollPoller::updateChannel(Channel* channel)
{
    struct epoll_event event;
    event.events = channel->events();
    event.data.ptr = channel;
    int fd = channel->fd();

    if (channel is new)
        ::epoll_ctl(epollfd_, EPOLL_CTL_ADD, fd, &event);
    else
        ::epoll_ctl(epollfd_, EPOLL_CTL_MOD, fd, &event);
}
```

* 通过 `epoll_ctl` 注册或修改监听
* 关注 `listenFd` 上的 `EPOLLIN` 事件（表示有新连接到来）


## 📌 📊 完整调用链整理成图

```
Acceptor::listen()
  ├── Socket::listen()            // 内核监听 listenFd
  └── Channel::enableReading()    // 关注 EPOLLIN 事件
        └── Channel::update()
              └── EventLoop::updateChannel()
                    └── EpollPoller::updateChannel()
                          └── epoll_ctl(ADD / MOD)
```


## 📌 📖 整体作用总结

👉 `Acceptor::listen()` 完成两个核心任务：

* **让内核监听 listenFd**，准备接受客户端连接
* **把 listenFd 注册到 epoll**，关注 `EPOLLIN` 事件（新连接到来）

当有新连接时，epoll 就会返回这个 `listenFd`，然后触发 Acceptor 注册的 `handleRead()` 回调，进入**连接建立流程**。


